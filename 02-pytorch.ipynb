{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02 PyTorch Autograd\n",
    "=====================\n",
    "### Date: Jan 9 2018\n",
    "### Author: Farahana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us learn simple use of autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(tc.Tensor([1]), requires_grad=True) #define x =1\n",
    "w = Variable(tc.Tensor([2]), requires_grad=True) #define w =2\n",
    "b = Variable(tc.Tensor([3]), requires_grad=True) #define b =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = w * x + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # to compute gradient of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn) # x is created by us, so no grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x has the gradient, resulted from y.backward() \n",
    "# and when the flag of requires_grad is true\n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x7fd599910240>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn) # y is a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad) # y is not created by us, thus no grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Autograd is an automatic differentiation module in pytorch. \n",
    "* It automates the backward computation in neural network. \n",
    "* Imagine a computaional graph with nodes and edges. \n",
    "Nodes are the tensors while edges are the functions of computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recap the previous example in [**01 Introduction into PyTorch**](01-pytorch.ipynb) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **N** is the batch size\n",
    "* **D_i** is the input dimension while **D_out** is the output dimension\n",
    "* **H** is the hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the example\n",
    "N, D_in, H, D_out = 24, 1000, 100, 4\n",
    "learning_rate = 1e-6\n",
    "\n",
    "dtype = tc.cuda.FloatTensor\n",
    "# dtype = tc.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the input and expected output which the gradient is not required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(tc.randn(N, D_in).type(dtype), requires_grad=False) # input\n",
    "y = Variable(tc.randn(N, D_out).type(dtype), requires_grad=False) # output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing weights randomly and set for gradient requirement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Variable(tc.randn(D_in, H).type(dtype), requires_grad=True) # weights_1\n",
    "w2 = Variable(tc.randn(H, D_out).type(dtype), requires_grad=True) # weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7644135.5\n",
      "1 2262797.5\n",
      "2 1502437.0\n",
      "3 1022621.3125\n",
      "4 708579.5625\n",
      "5 498156.8125\n",
      "6 354510.90625\n",
      "7 254962.6875\n",
      "8 185074.8125\n",
      "9 135411.265625\n",
      "10 99775.046875\n",
      "11 73982.2421875\n",
      "12 55163.1484375\n",
      "13 41348.67578125\n",
      "14 31142.341796875\n",
      "15 23559.6015625\n",
      "16 18010.763671875\n",
      "17 13844.4775390625\n",
      "18 10684.201171875\n",
      "19 8275.9228515625\n",
      "20 6435.82080078125\n",
      "21 5022.36572265625\n",
      "22 3930.627685546875\n",
      "23 3084.525390625\n",
      "24 2426.66162109375\n",
      "25 1913.693603515625\n",
      "26 1512.5338134765625\n",
      "27 1197.9970703125\n",
      "28 951.3430786132812\n",
      "29 757.2006225585938\n",
      "30 603.8499755859375\n",
      "31 482.4577331542969\n",
      "32 386.1450500488281\n",
      "33 309.58251953125\n",
      "34 248.59622192382812\n",
      "35 199.9409942626953\n",
      "36 161.04180908203125\n",
      "37 129.8955841064453\n",
      "38 104.91561126708984\n",
      "39 84.84761047363281\n",
      "40 68.7056884765625\n",
      "41 55.702171325683594\n",
      "42 45.210269927978516\n",
      "43 36.7368049621582\n",
      "44 29.884475708007812\n",
      "45 24.333560943603516\n",
      "46 19.833486557006836\n",
      "47 16.1809024810791\n",
      "48 13.214195251464844\n",
      "49 10.798812866210938\n",
      "50 8.834538459777832\n",
      "51 7.231388092041016\n",
      "52 5.9257283210754395\n",
      "53 4.858968257904053\n",
      "54 3.9872403144836426\n",
      "55 3.273406982421875\n",
      "56 2.6896207332611084\n",
      "57 2.2112178802490234\n",
      "58 1.819504737854004\n",
      "59 1.4977132081985474\n",
      "60 1.233473300933838\n",
      "61 1.0172090530395508\n",
      "62 0.8387460708618164\n",
      "63 0.6919623613357544\n",
      "64 0.571232795715332\n",
      "65 0.4717916250228882\n",
      "66 0.38983508944511414\n",
      "67 0.3221639096736908\n",
      "68 0.2665041983127594\n",
      "69 0.2204352617263794\n",
      "70 0.1825181394815445\n",
      "71 0.15113767981529236\n",
      "72 0.12516075372695923\n",
      "73 0.10376878827810287\n",
      "74 0.08600100874900818\n",
      "75 0.07131730020046234\n",
      "76 0.059173982590436935\n",
      "77 0.049134619534015656\n",
      "78 0.04079168289899826\n",
      "79 0.0338515043258667\n",
      "80 0.028124390169978142\n",
      "81 0.023383595049381256\n",
      "82 0.019435305148363113\n",
      "83 0.01614890806376934\n",
      "84 0.013446060009300709\n",
      "85 0.011193628422915936\n",
      "86 0.00932537391781807\n",
      "87 0.007772601675242186\n",
      "88 0.0064689223654568195\n",
      "89 0.005396449472755194\n",
      "90 0.004503077361732721\n",
      "91 0.003762798849493265\n",
      "92 0.0031395042315125465\n",
      "93 0.00262433965690434\n",
      "94 0.002192688873037696\n",
      "95 0.0018463048618286848\n",
      "96 0.0015468077035620809\n",
      "97 0.0013042003847658634\n",
      "98 0.0010972393210977316\n",
      "99 0.0009231190197169781\n",
      "100 0.0007883846410550177\n",
      "101 0.0006684028776362538\n",
      "102 0.0005709084798581898\n",
      "103 0.0004866202361881733\n",
      "104 0.00042041720007546246\n",
      "105 0.00036122981691733\n",
      "106 0.000311352894641459\n",
      "107 0.00026885780971497297\n",
      "108 0.00023204322496894747\n",
      "109 0.00020067111472599208\n",
      "110 0.000176685192855075\n",
      "111 0.0001537692587589845\n",
      "112 0.0001349061931250617\n",
      "113 0.0001185073924716562\n",
      "114 0.00010603809641906992\n",
      "115 9.350416075903922e-05\n",
      "116 8.32735895528458e-05\n",
      "117 7.449316763086244e-05\n",
      "118 6.689336441922933e-05\n",
      "119 5.997308835503645e-05\n",
      "120 5.380194852477871e-05\n",
      "121 4.814003477804363e-05\n",
      "122 4.3683226977009326e-05\n",
      "123 3.972790000261739e-05\n",
      "124 3.595259477151558e-05\n",
      "125 3.261309757363051e-05\n",
      "126 2.9851540602976456e-05\n",
      "127 2.7420650440035388e-05\n",
      "128 2.518384462746326e-05\n",
      "129 2.2876027287566103e-05\n",
      "130 2.102699363604188e-05\n",
      "131 1.9220475223846734e-05\n",
      "132 1.7814601960708387e-05\n",
      "133 1.6282847354887053e-05\n",
      "134 1.5002311556600034e-05\n",
      "135 1.3977773051010445e-05\n",
      "136 1.2962134860572405e-05\n",
      "137 1.2063675058016088e-05\n",
      "138 1.1180477486050222e-05\n",
      "139 1.0435959666210692e-05\n",
      "140 9.801703527045902e-06\n",
      "141 9.14864085643785e-06\n",
      "142 8.518988579453435e-06\n",
      "143 8.022660040296614e-06\n",
      "144 7.38353901397204e-06\n",
      "145 6.9698248807981145e-06\n",
      "146 6.491058229585178e-06\n",
      "147 6.160884822747903e-06\n",
      "148 5.766795766248833e-06\n",
      "149 5.44705744687235e-06\n",
      "150 5.236437118583126e-06\n",
      "151 4.996787538402714e-06\n",
      "152 4.742661076306831e-06\n",
      "153 4.509158770815702e-06\n",
      "154 4.273329977877438e-06\n",
      "155 4.103352239326341e-06\n",
      "156 3.922195901395753e-06\n",
      "157 3.703474931171513e-06\n",
      "158 3.5694986308953958e-06\n",
      "159 3.405127472433378e-06\n",
      "160 3.2717134672566317e-06\n",
      "161 3.1807608138478827e-06\n",
      "162 3.0636749670520658e-06\n",
      "163 2.945713049484766e-06\n",
      "164 2.8650824788201135e-06\n",
      "165 2.741067419265164e-06\n",
      "166 2.646483380885911e-06\n",
      "167 2.5349349925818387e-06\n",
      "168 2.465573743393179e-06\n",
      "169 2.3526217773905955e-06\n",
      "170 2.273188556500827e-06\n",
      "171 2.2316462491289712e-06\n",
      "172 2.1814450974488864e-06\n",
      "173 2.1048660983069567e-06\n",
      "174 2.0515601590886945e-06\n",
      "175 2.0024219793413067e-06\n",
      "176 1.921767761814408e-06\n",
      "177 1.8722378172242315e-06\n",
      "178 1.866427965069306e-06\n",
      "179 1.8183080783273908e-06\n",
      "180 1.7507774145997246e-06\n",
      "181 1.6983581190288533e-06\n",
      "182 1.6315492530338815e-06\n",
      "183 1.57867589223315e-06\n",
      "184 1.5698735751357162e-06\n",
      "185 1.533655108687526e-06\n",
      "186 1.4805848422838608e-06\n",
      "187 1.4431644785872777e-06\n",
      "188 1.4030570127943065e-06\n",
      "189 1.3614067029266153e-06\n",
      "190 1.335845013272774e-06\n",
      "191 1.333427803729137e-06\n",
      "192 1.3028680996285402e-06\n",
      "193 1.2603697996382834e-06\n",
      "194 1.2376716540529742e-06\n",
      "195 1.2256602985871723e-06\n",
      "196 1.1903157428605482e-06\n",
      "197 1.1870358775922796e-06\n",
      "198 1.1783647551055765e-06\n",
      "199 1.153873085968371e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(200):\n",
    "    # As weigths had the True flagged for gradient requirement, \n",
    "    # computations do not need references of backward pass, \n",
    "    # the computation of gradients are automated\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Initialized the loss to get the gradients\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    # The loss has become of the Variables as it uses y and y_pred in computation. \n",
    "    # .grad_fn and .backward()is available.\n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    # The supposed backward computations to get the gradients\n",
    "    # grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    # grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    # grad_h = grad_h_relu.clone()\n",
    "    # grad_h[h < 0] = 0\n",
    "    # grad_w1 = x.t().mm(grad_h)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradients only available for weights because only Weights (w1 and w2) got the reguires_grad as True\n",
    "    # Update the weights by substracting the gradients: w = w - learning_rate*w_gradients\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    \n",
    "    # Zeroing the gradients after its use\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us examine the expected output vs the predicted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0588 -0.1549 -0.9363 -0.6117\n",
      "-0.5614  0.2786 -0.6237  0.9257\n",
      " 0.4124  1.0885 -0.3523 -0.1353\n",
      "-0.6585  1.9010  0.4739  0.3442\n",
      " 0.7310  0.3233 -0.5113 -0.4284\n",
      "-0.4787  0.6141 -0.1065 -0.7604\n",
      " 0.9709  0.5020  1.8626 -1.3757\n",
      "-0.5717  0.4749  1.9955  0.1409\n",
      " 0.1185 -2.0504 -1.4487  1.1535\n",
      " 2.2414 -0.0043  0.6162 -0.4058\n",
      " 2.0237 -0.5026 -1.1217  0.5577\n",
      " 1.7662  0.1556  0.2713  0.9467\n",
      "-0.6436  0.4181  0.8284 -0.9558\n",
      " 0.5169 -0.7242  1.9608 -0.7905\n",
      " 0.2504 -0.1324 -0.3770  1.2509\n",
      " 0.4430 -1.8406 -0.4133 -0.8317\n",
      " 2.4253  0.2787 -0.0510 -0.3843\n",
      "-1.6114  1.1748  1.9463  1.1621\n",
      "-0.3994 -0.5286  0.7286 -0.1246\n",
      "-1.0058 -0.8582 -0.5995 -0.6680\n",
      " 2.8242  0.3650  0.3401 -1.3912\n",
      "-1.0736 -0.7527  0.4383  0.3862\n",
      " 0.0242  0.2578  0.5763 -0.1229\n",
      " 1.1378 -0.3710  0.4106 -0.5006\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      " Variable containing:\n",
      "-0.0588 -0.1548 -0.9366 -0.6116\n",
      "-0.5614  0.2785 -0.6237  0.9255\n",
      " 0.4125  1.0884 -0.3521 -0.1355\n",
      "-0.6585  1.9009  0.4739  0.3441\n",
      " 0.7311  0.3234 -0.5115 -0.4283\n",
      "-0.4787  0.6142 -0.1065 -0.7604\n",
      " 0.9708  0.5019  1.8628 -1.3759\n",
      "-0.5719  0.4749  1.9957  0.1408\n",
      " 0.1185 -2.0503 -1.4488  1.1535\n",
      " 2.2415 -0.0044  0.6161 -0.4058\n",
      " 2.0239 -0.5025 -1.1219  0.5577\n",
      " 1.7660  0.1556  0.2712  0.9468\n",
      "-0.6435  0.4182  0.8282 -0.9556\n",
      " 0.5169 -0.7244  1.9610 -0.7907\n",
      " 0.2505 -0.1324 -0.3771  1.2510\n",
      " 0.4429 -1.8406 -0.4133 -0.8317\n",
      " 2.4252  0.2786 -0.0511 -0.3843\n",
      "-1.6113  1.1748  1.9464  1.1621\n",
      "-0.3993 -0.5284  0.7285 -0.1245\n",
      "-1.0058 -0.8582 -0.5995 -0.6679\n",
      " 2.8242  0.3650  0.3402 -1.3912\n",
      "-1.0734 -0.7529  0.4381  0.3861\n",
      " 0.0241  0.2578  0.5764 -0.1229\n",
      " 1.1377 -0.3709  0.4106 -0.5006\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it seems much easier to use Autograd with neural network, \n",
    "PyTorch has another module for neural network that makes life easier.\n",
    "Where we will discuss later in another part.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining other type of gradients with autograd.Function ###\n",
    "We have to define new **forward** and **backward** functions into new defined *subclass* of torch.autograd.Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(tc.autograd.Function):\n",
    "    \"\"\"\n",
    "    Relu is defined\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the new defined autograd function in previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new set of weights to test the new functions\n",
    "w1_new = Variable(tc.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2_new = Variable(tc.randn(H, D_out).type(dtype), requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5723820.0\n",
      "1 2114383.5\n",
      "2 1400284.875\n",
      "3 958165.6875\n",
      "4 671006.25\n",
      "5 479670.5625\n",
      "6 349039.03125\n",
      "7 257918.890625\n",
      "8 193252.640625\n",
      "9 146505.40625\n",
      "10 112228.109375\n",
      "11 86800.484375\n",
      "12 67709.09375\n",
      "13 53213.54296875\n",
      "14 42102.71875\n",
      "15 33523.3203125\n",
      "16 26840.30859375\n",
      "17 21596.056640625\n",
      "18 17454.263671875\n",
      "19 14163.9375\n",
      "20 11536.53515625\n",
      "21 9427.9267578125\n",
      "22 7728.888671875\n",
      "23 6355.03369140625\n",
      "24 5238.806640625\n",
      "25 4328.54638671875\n",
      "26 3584.089599609375\n",
      "27 2973.500244140625\n",
      "28 2471.4169921875\n",
      "29 2057.49365234375\n",
      "30 1715.487060546875\n",
      "31 1432.366455078125\n",
      "32 1197.55126953125\n",
      "33 1002.6358032226562\n",
      "34 840.2159423828125\n",
      "35 704.8309326171875\n",
      "36 591.8270874023438\n",
      "37 497.4017639160156\n",
      "38 418.3902893066406\n",
      "39 352.1930847167969\n",
      "40 296.6844787597656\n",
      "41 250.0999298095703\n",
      "42 210.95851135253906\n",
      "43 178.05197143554688\n",
      "44 150.35897827148438\n",
      "45 127.04344940185547\n",
      "46 107.39552307128906\n",
      "47 90.8292007446289\n",
      "48 76.85138702392578\n",
      "49 65.05194854736328\n",
      "50 55.085792541503906\n",
      "51 46.662349700927734\n",
      "52 39.54330062866211\n",
      "53 33.519081115722656\n",
      "54 28.424198150634766\n",
      "55 24.113954544067383\n",
      "56 20.459196090698242\n",
      "57 17.363759994506836\n",
      "58 14.742497444152832\n",
      "59 12.519140243530273\n",
      "60 10.63446044921875\n",
      "61 9.034477233886719\n",
      "62 7.678776264190674\n",
      "63 6.527225971221924\n",
      "64 5.548834323883057\n",
      "65 4.7201433181762695\n",
      "66 4.014876842498779\n",
      "67 3.415534019470215\n",
      "68 2.906721830368042\n",
      "69 2.4743268489837646\n",
      "70 2.106630325317383\n",
      "71 1.7935981750488281\n",
      "72 1.5276105403900146\n",
      "73 1.3011741638183594\n",
      "74 1.1086387634277344\n",
      "75 0.9445598125457764\n",
      "76 0.8052495718002319\n",
      "77 0.6864635944366455\n",
      "78 0.5851901769638062\n",
      "79 0.49904853105545044\n",
      "80 0.4257926940917969\n",
      "81 0.363237589597702\n",
      "82 0.3099251985549927\n",
      "83 0.2644713222980499\n",
      "84 0.22581735253334045\n",
      "85 0.192767933011055\n",
      "86 0.16468551754951477\n",
      "87 0.1406581848859787\n",
      "88 0.12006396055221558\n",
      "89 0.10248969495296478\n",
      "90 0.08765961229801178\n",
      "91 0.07494854182004929\n",
      "92 0.06406305730342865\n",
      "93 0.05481123924255371\n",
      "94 0.04687182605266571\n",
      "95 0.04006342962384224\n",
      "96 0.03429514169692993\n",
      "97 0.02933545596897602\n",
      "98 0.025143392384052277\n",
      "99 0.021541325375437737\n",
      "100 0.018429376184940338\n",
      "101 0.015753204002976418\n",
      "102 0.01350320316851139\n",
      "103 0.011581148020923138\n",
      "104 0.009933157823979855\n",
      "105 0.008499173447489738\n",
      "106 0.007303694728761911\n",
      "107 0.006272957194596529\n",
      "108 0.005389938596636057\n",
      "109 0.004636680707335472\n",
      "110 0.00397759722545743\n",
      "111 0.003429515054449439\n",
      "112 0.002958644414320588\n",
      "113 0.002542045433074236\n",
      "114 0.0022004954516887665\n",
      "115 0.0019023229833692312\n",
      "116 0.0016447891248390079\n",
      "117 0.001426330185495317\n",
      "118 0.001237543416209519\n",
      "119 0.0010742765152826905\n",
      "120 0.0009353646892122924\n",
      "121 0.0008163581369444728\n",
      "122 0.0007119961082935333\n",
      "123 0.0006217430345714092\n",
      "124 0.0005477846716530621\n",
      "125 0.0004829553363379091\n",
      "126 0.00042368099093437195\n",
      "127 0.0003745948779396713\n",
      "128 0.0003327269514556974\n",
      "129 0.00029581665876321495\n",
      "130 0.0002628159709274769\n",
      "131 0.00023596963728778064\n",
      "132 0.00021013469086028636\n",
      "133 0.00018758118676487356\n",
      "134 0.00016781491285655648\n",
      "135 0.0001511015580035746\n",
      "136 0.00013598700752481818\n",
      "137 0.00012321991380304098\n",
      "138 0.00011114918743260205\n",
      "139 0.00010112715244758874\n",
      "140 9.198611951433122e-05\n",
      "141 8.373047603527084e-05\n",
      "142 7.650643965462223e-05\n",
      "143 6.996079901000485e-05\n",
      "144 6.410990317817777e-05\n",
      "145 5.8181856729788706e-05\n",
      "146 5.356810652301647e-05\n",
      "147 4.9065478378906846e-05\n",
      "148 4.525816984823905e-05\n",
      "149 4.175212234258652e-05\n",
      "150 3.8575126382056624e-05\n",
      "151 3.567430394468829e-05\n",
      "152 3.278203075751662e-05\n",
      "153 3.043704418814741e-05\n",
      "154 2.7930431315326132e-05\n",
      "155 2.605936060717795e-05\n",
      "156 2.3950213289936073e-05\n",
      "157 2.2378129870048724e-05\n",
      "158 2.0892453903798014e-05\n",
      "159 1.9424593119765632e-05\n",
      "160 1.8131106116925366e-05\n",
      "161 1.7154459783341736e-05\n",
      "162 1.6030542610678822e-05\n",
      "163 1.502114628237905e-05\n",
      "164 1.4139869563223328e-05\n",
      "165 1.3313445379026234e-05\n",
      "166 1.2425261047610547e-05\n",
      "167 1.1730526239261962e-05\n",
      "168 1.1108739272458479e-05\n",
      "169 1.0506583748792764e-05\n",
      "170 9.991946171794552e-06\n",
      "171 9.439561836188659e-06\n",
      "172 8.899107342585921e-06\n",
      "173 8.41985547594959e-06\n",
      "174 7.82941424404271e-06\n",
      "175 7.382995590887731e-06\n",
      "176 7.126668606360909e-06\n",
      "177 6.802170446462696e-06\n",
      "178 6.453219612012617e-06\n",
      "179 6.231853603821946e-06\n",
      "180 5.910688287258381e-06\n",
      "181 5.648019396176096e-06\n",
      "182 5.424755272542825e-06\n",
      "183 5.176140803087037e-06\n",
      "184 4.879758762399433e-06\n",
      "185 4.638829068426276e-06\n",
      "186 4.419272499944782e-06\n",
      "187 4.21510640080669e-06\n",
      "188 4.050157713209046e-06\n",
      "189 3.865253802359803e-06\n",
      "190 3.682044280139962e-06\n",
      "191 3.552622956703999e-06\n",
      "192 3.454018042248208e-06\n",
      "193 3.349880444147857e-06\n",
      "194 3.1930878776620375e-06\n",
      "195 3.028683522643405e-06\n",
      "196 2.9299192192411283e-06\n",
      "197 2.795507953123888e-06\n",
      "198 2.7132498416904127e-06\n",
      "199 2.6054747195303207e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(200):\n",
    "    # defining the function to be used\n",
    "    relu = MyReLU()\n",
    "    \n",
    "    y_pred = relu(x.mm(w1_new)).mm(w2_new)\n",
    "     \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    w1_new.data -= learning_rate * w1_new.grad.data\n",
    "    w2_new.data -= learning_rate * w2_new.grad.data\n",
    "\n",
    "    w1_new.grad.data.zero_()\n",
    "    w2_new.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0588 -0.1549 -0.9363 -0.6117\n",
      "-0.5614  0.2786 -0.6237  0.9257\n",
      " 0.4124  1.0885 -0.3523 -0.1353\n",
      "-0.6585  1.9010  0.4739  0.3442\n",
      " 0.7310  0.3233 -0.5113 -0.4284\n",
      "-0.4787  0.6141 -0.1065 -0.7604\n",
      " 0.9709  0.5020  1.8626 -1.3757\n",
      "-0.5717  0.4749  1.9955  0.1409\n",
      " 0.1185 -2.0504 -1.4487  1.1535\n",
      " 2.2414 -0.0043  0.6162 -0.4058\n",
      " 2.0237 -0.5026 -1.1217  0.5577\n",
      " 1.7662  0.1556  0.2713  0.9467\n",
      "-0.6436  0.4181  0.8284 -0.9558\n",
      " 0.5169 -0.7242  1.9608 -0.7905\n",
      " 0.2504 -0.1324 -0.3770  1.2509\n",
      " 0.4430 -1.8406 -0.4133 -0.8317\n",
      " 2.4253  0.2787 -0.0510 -0.3843\n",
      "-1.6114  1.1748  1.9463  1.1621\n",
      "-0.3994 -0.5286  0.7286 -0.1246\n",
      "-1.0058 -0.8582 -0.5995 -0.6680\n",
      " 2.8242  0.3650  0.3401 -1.3912\n",
      "-1.0736 -0.7527  0.4383  0.3862\n",
      " 0.0242  0.2578  0.5763 -0.1229\n",
      " 1.1378 -0.3710  0.4106 -0.5006\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      " Variable containing:\n",
      "-0.0589 -0.1552 -0.9364 -0.6117\n",
      "-0.5614  0.2786 -0.6237  0.9257\n",
      " 0.4124  1.0884 -0.3524 -0.1352\n",
      "-0.6584  1.9008  0.4737  0.3441\n",
      " 0.7311  0.3233 -0.5114 -0.4284\n",
      "-0.4788  0.6142 -0.1064 -0.7604\n",
      " 0.9710  0.5019  1.8622 -1.3761\n",
      "-0.5716  0.4750  1.9955  0.1409\n",
      " 0.1183 -2.0502 -1.4485  1.1536\n",
      " 2.2409 -0.0047  0.6157 -0.4060\n",
      " 2.0239 -0.5025 -1.1215  0.5577\n",
      " 1.7662  0.1557  0.2715  0.9467\n",
      "-0.6436  0.4181  0.8285 -0.9557\n",
      " 0.5170 -0.7241  1.9608 -0.7905\n",
      " 0.2503 -0.1322 -0.3768  1.2510\n",
      " 0.4431 -1.8405 -0.4133 -0.8317\n",
      " 2.4253  0.2788 -0.0509 -0.3844\n",
      "-1.6113  1.1747  1.9463  1.1620\n",
      "-0.3997 -0.5287  0.7286 -0.1245\n",
      "-1.0060 -0.8581 -0.5992 -0.6680\n",
      " 2.8241  0.3651  0.3403 -1.3911\n",
      "-1.0733 -0.7528  0.4379  0.3859\n",
      " 0.0246  0.2579  0.5763 -0.1228\n",
      " 1.1378 -0.3711  0.4105 -0.5005\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
