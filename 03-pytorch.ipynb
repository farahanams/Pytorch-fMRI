{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03 PyTorch nn \n",
    "=====================\n",
    "### Date: Jan 10 2018\n",
    "### Author: Farahana\n",
    "\n",
    "`Added Source: https://github.com/farahanams/DeepLearningZeroToAll/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss the **nn** module. It provides higher-level abstraction just like **Keras**.\n",
    "* It defines a set of Modules of neural network layers such as Sequential module in Keras.\n",
    "* It also defines set of useful loss functions such as torch.nn.MSELoss.\n",
    "* In addition, this module also hold internal state of Variables that it works with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the previous example and define it using **nn.module**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the example\n",
    "N, D_in, H, D_out = 24, 1000, 100, 4\n",
    "learning_rate = 1e-3\n",
    "\n",
    "dtype = tc.cuda.FloatTensor\n",
    "\n",
    "x = Variable(tc.randn(N, D_in).type(dtype), requires_grad=True) # input\n",
    "y = Variable(tc.randn(N, D_out).type(dtype), requires_grad=False) # output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define abstraction of the neural network that we previously have used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(D_in,H), nn.ReLU(), nn.Linear(H, D_out)).cuda() \n",
    "# execute using GPU with.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential** module takes a sequence of layers that could receive other **nn.module**.\n",
    "* **nn.Linear()** defines the first layer that receives input,**x** and outputs other Variables.\n",
    "* **nn.ReLU()** defines the activation layer that receives input from the first layer and outputs the clamped Variables.\n",
    "* **nn.Linear()** defines the second layer that receives clamped Variables and outputs other Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=100)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=4)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With defined model, **weights** are defined internally inside the model when used in the computaions. Now, we could define the loss functions using **nn.module**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${loss}(x, y)  = 1/n \\sum |x_i - y_i|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(size_average=False) # avoid size averaging with 'n' element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 75.6502914428711\n",
      "1 34.81554412841797\n",
      "2 18.56421661376953\n",
      "3 10.315837860107422\n",
      "4 5.834240436553955\n",
      "5 3.32458758354187\n",
      "6 1.9198143482208252\n",
      "7 1.1251261234283447\n",
      "8 0.6750346422195435\n",
      "9 0.413327693939209\n",
      "10 0.2575715482234955\n",
      "11 0.1632404923439026\n",
      "12 0.10521600395441055\n",
      "13 0.06888440251350403\n",
      "14 0.04575295001268387\n",
      "15 0.030809152871370316\n",
      "16 0.020993873476982117\n",
      "17 0.014418864622712135\n",
      "18 0.010001510381698608\n",
      "19 0.007000824902206659\n",
      "20 0.004943580832332373\n",
      "21 0.003518520388752222\n",
      "22 0.0025231579784303904\n",
      "23 0.0018224160885438323\n",
      "24 0.0013253279030323029\n",
      "25 0.0009702412644401193\n",
      "26 0.0007159261731430888\n",
      "27 0.0005317777977325022\n",
      "28 0.00039746647235006094\n",
      "29 0.00029889712459407747\n",
      "30 0.00022608648578170687\n",
      "31 0.00017198162095155567\n",
      "32 0.0001315425761276856\n",
      "33 0.00010112916061189026\n",
      "34 7.813461706973612e-05\n",
      "35 6.0649257648037747e-05\n",
      "36 4.7284633183153346e-05\n",
      "37 3.7018053262727335e-05\n",
      "38 2.9093120247125626e-05\n",
      "39 2.2945339878788218e-05\n",
      "40 1.8158823877456598e-05\n",
      "41 1.4413997632800601e-05\n",
      "42 1.1473816812213045e-05\n",
      "43 9.156276064459234e-06\n",
      "44 7.324921170948073e-06\n",
      "45 5.871815574209904e-06\n",
      "46 4.716590865427861e-06\n",
      "47 3.7952834190946305e-06\n",
      "48 3.0587234505219385e-06\n",
      "49 2.4684268282726407e-06\n",
      "50 1.994731974264141e-06\n",
      "51 1.613790232113388e-06\n",
      "52 1.3071889952698257e-06\n",
      "53 1.0598299695629976e-06\n",
      "54 8.59869089708809e-07\n",
      "55 6.982219815654389e-07\n",
      "56 5.673665555150365e-07\n",
      "57 4.612987254404288e-07\n",
      "58 3.7522474372053694e-07\n",
      "59 3.053542911857221e-07\n",
      "60 2.4866423586900055e-07\n",
      "61 2.0250244858743827e-07\n",
      "62 1.6502632149695273e-07\n",
      "63 1.3453005465180468e-07\n",
      "64 1.0965273844476542e-07\n",
      "65 8.939105811123227e-08\n",
      "66 7.294784154510126e-08\n",
      "67 5.9523106443748475e-08\n",
      "68 4.85770499381033e-08\n",
      "69 3.9648568872507894e-08\n",
      "70 3.235555467995255e-08\n",
      "71 2.6393681906711208e-08\n",
      "72 2.1559040774832283e-08\n",
      "73 1.7607824531751248e-08\n",
      "74 1.4378532142700351e-08\n",
      "75 1.1748362283015013e-08\n",
      "76 9.606488937663471e-09\n",
      "77 7.838049320696427e-09\n",
      "78 6.408775288235802e-09\n",
      "79 5.243800060839021e-09\n",
      "80 4.284695265255323e-09\n",
      "81 3.5025824463019717e-09\n",
      "82 2.8539266505589467e-09\n",
      "83 2.3452793040945608e-09\n",
      "84 1.916390157674641e-09\n",
      "85 1.571661356614129e-09\n",
      "86 1.2851476549613494e-09\n",
      "87 1.0519292104049782e-09\n",
      "88 8.627499270552619e-10\n",
      "89 7.074275609753045e-10\n",
      "90 5.807919145617291e-10\n",
      "91 4.738164860462746e-10\n",
      "92 3.941895698744702e-10\n",
      "93 3.212393961948834e-10\n",
      "94 2.668066600985952e-10\n",
      "95 2.1761592527980156e-10\n",
      "96 1.8122127409814226e-10\n",
      "97 1.4924030067309246e-10\n",
      "98 1.2420721395844936e-10\n",
      "99 1.0398578526027435e-10\n",
      "100 8.694697739564106e-11\n",
      "101 7.239190497054793e-11\n",
      "102 6.082721132338875e-11\n",
      "103 5.163867597413052e-11\n",
      "104 4.3985742131935623e-11\n",
      "105 3.7361450738737645e-11\n",
      "106 3.1497051494744355e-11\n",
      "107 2.806236666652584e-11\n",
      "108 2.4073085538867112e-11\n",
      "109 2.0949084481025615e-11\n",
      "110 1.8544174876233832e-11\n",
      "111 1.65993139372711e-11\n",
      "112 1.4944013040918414e-11\n",
      "113 1.322332694730921e-11\n",
      "114 1.2038156929627952e-11\n",
      "115 1.1145128223088996e-11\n",
      "116 1.0073894843309894e-11\n",
      "117 9.24212269215463e-12\n",
      "118 8.393454334343353e-12\n",
      "119 8.229627049272104e-12\n",
      "120 7.638925950126385e-12\n",
      "121 7.116932477374549e-12\n",
      "122 6.706961808849998e-12\n",
      "123 5.929708547097734e-12\n",
      "124 5.498511802121131e-12\n",
      "125 5.1648065164944246e-12\n",
      "126 4.891421035574384e-12\n",
      "127 4.562073375319331e-12\n",
      "128 4.53506026135142e-12\n",
      "129 4.18023298268122e-12\n",
      "130 4.196227133129726e-12\n",
      "131 3.85829606111554e-12\n",
      "132 3.7777285639972735e-12\n",
      "133 3.37425270016678e-12\n",
      "134 3.2302359571911854e-12\n",
      "135 3.2015922031558564e-12\n",
      "136 3.025094497816072e-12\n",
      "137 2.727055126855449e-12\n",
      "138 2.672147659393831e-12\n",
      "139 2.543944655625241e-12\n",
      "140 2.467949889589649e-12\n",
      "141 2.463384097400878e-12\n",
      "142 2.4478062805866063e-12\n",
      "143 2.217983175595295e-12\n",
      "144 2.1979714055764266e-12\n",
      "145 2.0895858827973957e-12\n",
      "146 1.964130681014753e-12\n",
      "147 1.8987246670765234e-12\n",
      "148 1.8581737711020896e-12\n",
      "149 1.7418223981213732e-12\n",
      "150 1.6597214054503429e-12\n",
      "151 1.6550862243225328e-12\n",
      "152 1.6525882225171262e-12\n",
      "153 1.6199615433809544e-12\n",
      "154 1.5669761495307188e-12\n",
      "155 1.6010877519623268e-12\n",
      "156 1.4957969758644385e-12\n",
      "157 1.3770447455929702e-12\n",
      "158 1.3730479427043196e-12\n",
      "159 1.3113750536863922e-12\n",
      "160 1.3157812513153733e-12\n",
      "161 1.2993638283387288e-12\n",
      "162 1.1550903462886897e-12\n",
      "163 1.1930114014735427e-12\n",
      "164 1.188723165040928e-12\n",
      "165 1.1344540758184696e-12\n",
      "166 1.1058241995709484e-12\n",
      "167 1.0617622232811375e-12\n",
      "168 1.0582372651779526e-12\n",
      "169 1.076299206009823e-12\n",
      "170 1.0092417353224636e-12\n",
      "171 9.70016168083676e-13\n",
      "172 9.772881288949709e-13\n",
      "173 9.907565219624548e-13\n",
      "174 9.197022483864448e-13\n",
      "175 8.268876577378903e-13\n",
      "176 8.366576203545917e-13\n",
      "177 8.483496565826754e-13\n",
      "178 8.996974714715888e-13\n",
      "179 9.058592092582585e-13\n",
      "180 8.590077976190769e-13\n",
      "181 8.267766354354278e-13\n",
      "182 8.523117650018064e-13\n",
      "183 8.123437361153008e-13\n",
      "184 7.962593800460438e-13\n",
      "185 8.155772606745215e-13\n",
      "186 7.824232256016528e-13\n",
      "187 7.635771897586408e-13\n",
      "188 7.648539462369597e-13\n",
      "189 7.108971072401771e-13\n",
      "190 7.108971072401771e-13\n",
      "191 7.099811732448613e-13\n",
      "192 6.690356276796428e-13\n",
      "193 6.568786855599973e-13\n",
      "194 6.397812509807699e-13\n",
      "195 6.662045589668486e-13\n",
      "196 6.652331138203016e-13\n",
      "197 6.61090594159669e-13\n",
      "198 7.117167640825761e-13\n",
      "199 6.185968078921411e-13\n"
     ]
    }
   ],
   "source": [
    "for t in range(200):\n",
    "    # The sequential model is called like a function that receives input, x \n",
    "    # and output the predicted output, y_pred.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # calculating the loss with x=y_pred and y=y_expected\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    # zeroing the gradients each time before backward computation. Replacing the 'w1_new.grad.data.zero_()'\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Now, each set of weights are called with nn.module.parameters() and updated\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we will check the expected vs predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.6496 -0.4227 -1.1463 -0.3795\n",
      " 0.0976 -0.0083 -1.1085 -0.8823\n",
      "-2.3640  0.1273 -0.8051 -1.0918\n",
      " 0.2950 -0.9567  0.0137 -1.0619\n",
      "-0.0589  0.1522  0.7067 -1.0940\n",
      "-0.9402 -1.1008  0.1465 -1.0812\n",
      " 0.9588 -1.7039  0.3469  1.1945\n",
      "-0.0608 -0.4277  0.5560  0.5204\n",
      "-1.1139  0.8071  0.3739  0.3287\n",
      "-0.1438  0.1531  1.5651 -0.1208\n",
      "-0.3952  0.1708 -0.9400 -0.4823\n",
      "-1.1603 -1.8515  1.2225  0.4915\n",
      " 1.5582  1.7589  1.3023  1.3325\n",
      "-0.4448  1.2730 -0.7684 -0.1216\n",
      "-1.4069 -0.5072  0.3938 -0.0016\n",
      "-1.2708  0.4589  0.8487  0.4459\n",
      "-1.0207  0.9144 -1.0939 -1.5429\n",
      "-0.7213 -0.9262 -0.6361  0.5749\n",
      "-1.0738 -0.3440  1.0316 -1.6731\n",
      "-0.3708  0.3138 -0.7776  1.8608\n",
      " 1.0102  0.4858 -0.5503 -1.0134\n",
      " 0.0428  0.5109 -0.2697  0.6546\n",
      " 0.6380  1.0079  0.2307 -0.5711\n",
      " 0.7821 -0.1168  0.4810  1.3921\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      " Variable containing:\n",
      "-0.6496 -0.4227 -1.1463 -0.3795\n",
      " 0.0976 -0.0083 -1.1085 -0.8823\n",
      "-2.3640  0.1273 -0.8051 -1.0918\n",
      " 0.2950 -0.9567  0.0137 -1.0619\n",
      "-0.0589  0.1522  0.7067 -1.0940\n",
      "-0.9402 -1.1008  0.1465 -1.0812\n",
      " 0.9588 -1.7039  0.3469  1.1945\n",
      "-0.0608 -0.4277  0.5560  0.5204\n",
      "-1.1139  0.8071  0.3739  0.3287\n",
      "-0.1438  0.1531  1.5651 -0.1208\n",
      "-0.3952  0.1708 -0.9400 -0.4823\n",
      "-1.1603 -1.8515  1.2225  0.4915\n",
      " 1.5582  1.7589  1.3023  1.3325\n",
      "-0.4448  1.2730 -0.7684 -0.1216\n",
      "-1.4069 -0.5072  0.3938 -0.0016\n",
      "-1.2708  0.4589  0.8487  0.4459\n",
      "-1.0207  0.9144 -1.0939 -1.5429\n",
      "-0.7213 -0.9262 -0.6361  0.5749\n",
      "-1.0738 -0.3440  1.0316 -1.6731\n",
      "-0.3708  0.3138 -0.7776  1.8608\n",
      " 1.0102  0.4858 -0.5503 -1.0134\n",
      " 0.0428  0.5109 -0.2697  0.6546\n",
      " 0.6380  1.0079  0.2307 -0.5711\n",
      " 0.7821 -0.1168  0.4810  1.3921\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have 3 types of implementation to the first example introduced. \n",
    "Let us learn new and simple example using the three types of modules; **Variable**, **Autograd** and **nn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with PyTorch ###\n",
    "Linear regression is a basic of neural network layer. But, first let us initialize the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, D_out = 60, 1, 1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "dtype = tc.cuda.FloatTensor\n",
    "\n",
    "x = Variable(tc.Tensor([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], \n",
    "                        [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], \n",
    "                        [7.997], [3.1]]).type(dtype), requires_grad=False) #define x\n",
    "\n",
    "y = Variable(tc.Tensor([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366],\n",
    "                        [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], \n",
    "                        [2.904], [1.3]]).type(dtype), requires_grad=False) #define y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining linear regression using **nn.Linear** module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = xw + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = nn.Linear(1,1, bias =True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define activation function using sigmoid function of **nn.Sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let us put it together to become a model using **nn.Sequential**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1)\n",
      "  (1): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "modelLR = nn.Sequential(LR, sigmoid)\n",
    "print (modelLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us use same loss function from previous example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(size_average=False) # avoid size averaging with 'n' element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let us train the model with 5000 epochs.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34.71846008300781\n",
      "100 34.64232635498047\n",
      "200 34.60383605957031\n",
      "300 34.58026885986328\n",
      "400 34.56421661376953\n",
      "500 34.55250930786133\n",
      "600 34.54356384277344\n",
      "700 34.53648376464844\n",
      "800 34.530731201171875\n",
      "900 34.52595138549805\n",
      "1000 34.52191162109375\n",
      "1100 34.51844787597656\n",
      "1200 34.51544189453125\n",
      "1300 34.51280975341797\n",
      "1400 34.51047897338867\n",
      "1500 34.50840377807617\n",
      "1600 34.50653839111328\n",
      "1700 34.50485610961914\n",
      "1800 34.503326416015625\n",
      "1900 34.501930236816406\n",
      "2000 34.50065612792969\n",
      "2100 34.499481201171875\n",
      "2200 34.49839401245117\n",
      "2300 34.49739074707031\n",
      "2400 34.49645233154297\n",
      "2500 34.495582580566406\n",
      "2600 34.494773864746094\n",
      "2700 34.49401092529297\n",
      "2800 34.49329376220703\n",
      "2900 34.49262237548828\n",
      "3000 34.49198913574219\n",
      "3100 34.49138641357422\n",
      "3200 34.490821838378906\n",
      "3300 34.49028396606445\n",
      "3400 34.48977279663086\n",
      "3500 34.489288330078125\n",
      "3600 34.48883056640625\n",
      "3700 34.48838806152344\n",
      "3800 34.487972259521484\n",
      "3900 34.48756790161133\n",
      "4000 34.4871826171875\n",
      "4100 34.486820220947266\n",
      "4200 34.48646926879883\n",
      "4300 34.48612976074219\n",
      "4400 34.485809326171875\n",
      "4500 34.485496520996094\n",
      "4600 34.485198974609375\n",
      "4700 34.48490905761719\n",
      "4800 34.48463439941406\n",
      "4900 34.48436737060547\n"
     ]
    }
   ],
   "source": [
    "for t in range(5000):\n",
    "    # Calling the model\n",
    "    y_pred = modelLR(x)\n",
    "    \n",
    "    # calculating the loss with x=y_pred and y=y_expected\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t%100 == 0:\n",
    "        print(t, loss.data[0])\n",
    "    \n",
    "    # zeroing the gradients each time before backward computation. Replacing the 'w1_new.grad.data.zero_()'\n",
    "    modelLR.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Now, each set of weights are called with nn.module.parameters() and updated\n",
    "    for param in modelLR.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a very long interations, it seems the loss is very big and hardly converge, let us check the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9974\n",
      " 0.9995\n",
      " 0.9999\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9993\n",
      " 1.0000\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9855\n",
      " 1.0000\n",
      " 1.0000\n",
      " 0.9999\n",
      " 1.0000\n",
      " 0.9965\n",
      "[torch.cuda.FloatTensor of size 15x1 (GPU 0)]\n",
      " Variable containing:\n",
      " 1.7000\n",
      " 2.7600\n",
      " 2.0900\n",
      " 3.1900\n",
      " 1.6940\n",
      " 1.5730\n",
      " 3.3660\n",
      " 2.5960\n",
      " 2.5300\n",
      " 1.2210\n",
      " 2.8270\n",
      " 3.4650\n",
      " 1.6500\n",
      " 2.9040\n",
      " 1.3000\n",
      "[torch.cuda.FloatTensor of size 15x1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because we set the activation function to be sigmoid and the y_pred is set to be between [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us redefine the model by using only **LR=nn.Linear()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1053.002197265625\n",
      "1000 2.533729076385498\n",
      "2000 2.533728837966919\n",
      "3000 2.533728837966919\n",
      "4000 2.533728837966919\n",
      "5000 2.533728837966919\n",
      "6000 2.533728837966919\n",
      "7000 2.533728837966919\n",
      "8000 2.533728837966919\n",
      "9000 2.533728837966919\n",
      "10000 2.533728837966919\n",
      "11000 2.533728837966919\n",
      "12000 2.533728837966919\n",
      "13000 2.533728837966919\n",
      "14000 2.533728837966919\n",
      "15000 2.533728837966919\n",
      "16000 2.533728837966919\n",
      "17000 2.533728837966919\n",
      "18000 2.533728837966919\n",
      "19000 2.533728837966919\n",
      "20000 2.533728837966919\n",
      "21000 2.533728837966919\n",
      "22000 2.533728837966919\n",
      "23000 2.533728837966919\n",
      "24000 2.533728837966919\n",
      "25000 2.533728837966919\n",
      "26000 2.533728837966919\n",
      "27000 2.533728837966919\n",
      "28000 2.533728837966919\n",
      "29000 2.533728837966919\n",
      "30000 2.533728837966919\n",
      "31000 2.533728837966919\n",
      "32000 2.533728837966919\n",
      "33000 2.533728837966919\n",
      "34000 2.533728837966919\n",
      "35000 2.533728837966919\n",
      "36000 2.533728837966919\n",
      "37000 2.533728837966919\n",
      "38000 2.533728837966919\n",
      "39000 2.533728837966919\n",
      "40000 2.533728837966919\n",
      "41000 2.533728837966919\n",
      "42000 2.533728837966919\n",
      "43000 2.533728837966919\n",
      "44000 2.533728837966919\n",
      "45000 2.533728837966919\n",
      "46000 2.533728837966919\n",
      "47000 2.533728837966919\n",
      "48000 2.533728837966919\n",
      "49000 2.533728837966919\n"
     ]
    }
   ],
   "source": [
    "for t in range(50000):\n",
    "    # Calling the model\n",
    "    y_pred = LR(x)\n",
    "    \n",
    "    # calculating the loss with x=y_pred and y=y_expected\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t%1000 == 0:\n",
    "        print(t, loss.data[0])\n",
    "    \n",
    "    # zeroing the gradients each time before backward computation. Replacing the 'w1_new.grad.data.zero_()'\n",
    "    LR.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Now, each set of weights are called with nn.module.parameters() and updated\n",
    "    for param in modelLR.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.6056\n",
      " 1.8916\n",
      " 2.1776\n",
      " 2.4922\n",
      " 2.5494\n",
      " 1.8313\n",
      " 3.2901\n",
      " 2.3549\n",
      " 2.7210\n",
      " 1.3111\n",
      " 2.5785\n",
      " 3.5532\n",
      " 2.1290\n",
      " 2.8268\n",
      " 1.5536\n",
      "[torch.cuda.FloatTensor of size 15x1 (GPU 0)]\n",
      " Variable containing:\n",
      " 1.7000\n",
      " 2.7600\n",
      " 2.0900\n",
      " 3.1900\n",
      " 1.6940\n",
      " 1.5730\n",
      " 3.3660\n",
      " 2.5960\n",
      " 2.5300\n",
      " 1.2210\n",
      " 2.8270\n",
      " 3.4650\n",
      " 1.6500\n",
      " 2.9040\n",
      " 1.3000\n",
      "[torch.cuda.FloatTensor of size 15x1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the prediction not converge. And this actually a good enough model for a linear regression.\n",
    "\n",
    "However, we could improve the model with optimization and regularization. We will learn this in next part using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = (x.data.cpu()).numpy()\n",
    "yy = (y.data.cpu()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = (y_pred.data.cpu()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3078ae3390>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXZIaZLhLUoP+H9kvQffw0V85AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3078b03f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "plt.plot(y_p, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3078a4fda0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3078a8ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "plt.scatter(yy, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
