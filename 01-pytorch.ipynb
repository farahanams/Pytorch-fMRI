{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Introduction into PyTorch \n",
    "\n",
    "### Date: Jan 8 2018\n",
    "### Author: Farahana\n",
    "\n",
    "Source : \n",
    "    1. https://github.com/jcjohnson/pytorch-examples\n",
    "    2. https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import torch as tc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "In numpy, it is np.ndarray, while in pytorch, it is tc.Tensor, To get random.rand, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10524064  0.55441554  0.63890575]\n",
      " [ 0.31140732  0.92061839  0.09061959]\n",
      " [ 0.52697401  0.84787463  0.4340737 ]\n",
      " [ 0.81852176  0.73008139  0.98265122]\n",
      " [ 0.84791687  0.51823401  0.25201279]]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(5,3)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7258  0.5079  0.5913\n",
      " 0.2510  0.9606  0.9861\n",
      " 0.4332  0.7519  0.6741\n",
      " 0.9888  0.6890  0.5213\n",
      " 0.4498  0.7290  0.2245\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x2 = tc.rand(5,3) #tc.rand() returns uniform distribution\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the torch tensor has the GPU computation. Simply change the datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = tc.cuda.FloatTensor\n",
    "# dtype = tc.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.1063 -0.2758  0.9599\n",
      " 1.1637 -1.0765  0.0800\n",
      " 1.2271 -0.4760  0.1590\n",
      "-1.9362 -0.0719 -0.0749\n",
      "-0.8377 -1.5127 -1.7474\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tc.randn(5,3).type(dtype) #tc.randn() returns normal distribution\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply use condition to run on GPU;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.1888  0.4248  1.0202\n",
      " 0.6308  1.1899  1.6353\n",
      " 1.1508  0.7552  0.5693\n",
      " 1.2280  0.9363  1.0905\n",
      " 1.1289  1.4109  1.5023\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tc.rand(5,3)\n",
    "y = tc.rand(5,3)\n",
    "\n",
    "if tc.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    total = x+y\n",
    "    \n",
    "print (total)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other tensor type on Torch. shown is FloatTensor and how to use it for GPU computation.\n",
    "\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable\n",
    "It is a thin wrapper around Torch Tensors. It records the operation applied into Tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create input and output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 24, 1000, 100, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(tc.randn(N, D_in).type(dtype), requires_grad=False) # input\n",
    "y = Variable(tc.randn(N, D_out).type(dtype), requires_grad=False) # output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buliding simple loss function to get weights for y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Variable(tc.randn(D_in, H).type(dtype), requires_grad=True) # weights_1\n",
    "w2 = Variable(tc.randn(H, D_out).type(dtype), requires_grad=True) # weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4394089.5\n",
      "1 1786397.25\n",
      "2 1225609.125\n",
      "3 863376.0625\n",
      "4 619790.75\n",
      "5 451727.875\n",
      "6 333100.59375\n",
      "7 248202.625\n",
      "8 186722.3125\n",
      "9 141566.671875\n",
      "10 108027.453125\n",
      "11 82892.1640625\n",
      "12 63915.59765625\n",
      "13 49500.7265625\n",
      "14 38515.0703125\n",
      "15 30122.875\n",
      "16 23647.798828125\n",
      "17 18619.017578125\n",
      "18 14697.9609375\n",
      "19 11630.669921875\n",
      "20 9224.2080078125\n",
      "21 7332.38037109375\n",
      "22 5844.6279296875\n",
      "23 4676.1005859375\n",
      "24 3748.11083984375\n",
      "25 3010.537841796875\n",
      "26 2421.616943359375\n",
      "27 1951.6900634765625\n",
      "28 1575.3828125\n",
      "29 1273.6552734375\n",
      "30 1030.9921875\n",
      "31 835.557861328125\n",
      "32 677.9508666992188\n",
      "33 550.6835327148438\n",
      "34 447.78271484375\n",
      "35 364.4840393066406\n",
      "36 296.96429443359375\n",
      "37 242.5496826171875\n",
      "38 198.32374572753906\n",
      "39 162.31854248046875\n",
      "40 132.96420288085938\n",
      "41 109.01695251464844\n",
      "42 89.44876098632812\n",
      "43 73.45408630371094\n",
      "44 60.36873245239258\n",
      "45 49.64921951293945\n",
      "46 40.86111831665039\n",
      "47 33.65228271484375\n",
      "48 27.733482360839844\n",
      "49 22.86996078491211\n",
      "50 18.871492385864258\n",
      "51 15.579492568969727\n",
      "52 12.87059497833252\n",
      "53 10.636991500854492\n",
      "54 8.79703140258789\n",
      "55 7.278286933898926\n",
      "56 6.024843215942383\n",
      "57 4.989536285400391\n",
      "58 4.134151458740234\n",
      "59 3.4269397258758545\n",
      "60 2.8409435749053955\n",
      "61 2.356539249420166\n",
      "62 1.955891728401184\n",
      "63 1.6240019798278809\n",
      "64 1.3485100269317627\n",
      "65 1.1201096773147583\n",
      "66 0.9308262467384338\n",
      "67 0.7739348411560059\n",
      "68 0.6434404253959656\n",
      "69 0.5353124141693115\n",
      "70 0.44539475440979004\n",
      "71 0.3706965446472168\n",
      "72 0.30875054001808167\n",
      "73 0.25690221786499023\n",
      "74 0.21390463411808014\n",
      "75 0.17823900282382965\n",
      "76 0.14854344725608826\n",
      "77 0.12399710714817047\n",
      "78 0.10333341360092163\n",
      "79 0.08615878224372864\n",
      "80 0.07183074206113815\n",
      "81 0.059939704835414886\n",
      "82 0.050073616206645966\n",
      "83 0.041766900569200516\n",
      "84 0.034874241799116135\n",
      "85 0.029064729809761047\n",
      "86 0.0242887232452631\n",
      "87 0.020259328186511993\n",
      "88 0.016928741708397865\n",
      "89 0.014122514985501766\n",
      "90 0.01180203165858984\n",
      "91 0.009856349788606167\n",
      "92 0.008257731795310974\n",
      "93 0.0068870228715240955\n",
      "94 0.00576730165630579\n",
      "95 0.004836248233914375\n",
      "96 0.004043856635689735\n",
      "97 0.0034036750439554453\n",
      "98 0.002862887689843774\n",
      "99 0.0023979702964425087\n",
      "100 0.0020170360803604126\n",
      "101 0.0017000740626826882\n",
      "102 0.001434055040590465\n",
      "103 0.0012134673306718469\n",
      "104 0.001030626124702394\n",
      "105 0.00087305111810565\n",
      "106 0.0007446154486387968\n",
      "107 0.0006351771298795938\n",
      "108 0.0005417752545326948\n",
      "109 0.000463187723653391\n",
      "110 0.00039830931928008795\n",
      "111 0.00034422686439938843\n",
      "112 0.00029710287344641984\n",
      "113 0.0002574684622231871\n",
      "114 0.00022370344959199429\n",
      "115 0.0001971430901903659\n",
      "116 0.00017256764112971723\n",
      "117 0.00015198629989754409\n",
      "118 0.00013436957669910043\n",
      "119 0.00011954468209296465\n",
      "120 0.00010643013229127973\n",
      "121 9.517081343801692e-05\n",
      "122 8.513660577591509e-05\n",
      "123 7.627883314853534e-05\n",
      "124 6.844665767857805e-05\n",
      "125 6.136941374279559e-05\n",
      "126 5.5512024118797854e-05\n",
      "127 5.002598118153401e-05\n",
      "128 4.550961602944881e-05\n",
      "129 4.1164028516504914e-05\n",
      "130 3.768243550439365e-05\n",
      "131 3.4068329114234075e-05\n",
      "132 3.12311458401382e-05\n",
      "133 2.8586615371750668e-05\n",
      "134 2.612339267216157e-05\n",
      "135 2.3831267753848806e-05\n",
      "136 2.1934161850367673e-05\n",
      "137 2.034255703620147e-05\n",
      "138 1.8904205717262812e-05\n",
      "139 1.748380236676894e-05\n",
      "140 1.6354968465748243e-05\n",
      "141 1.5170438018685672e-05\n",
      "142 1.4207696040102746e-05\n",
      "143 1.3296507859195117e-05\n",
      "144 1.2458479432098102e-05\n",
      "145 1.1609511602728162e-05\n",
      "146 1.0783417565107811e-05\n",
      "147 1.0081270374939777e-05\n",
      "148 9.391222192789428e-06\n",
      "149 8.798966518952511e-06\n",
      "150 8.312807949550916e-06\n",
      "151 7.840434591344092e-06\n",
      "152 7.367172656813636e-06\n",
      "153 6.918609869899228e-06\n",
      "154 6.5794743022706825e-06\n",
      "155 6.203214070410468e-06\n",
      "156 5.898947165405843e-06\n",
      "157 5.569747827394167e-06\n",
      "158 5.196281108510448e-06\n",
      "159 5.007023901271168e-06\n",
      "160 4.736736627819482e-06\n",
      "161 4.471561169339111e-06\n",
      "162 4.333307060733205e-06\n",
      "163 4.203008302283706e-06\n",
      "164 4.0113286559062544e-06\n",
      "165 3.8507391764142085e-06\n",
      "166 3.75315903511364e-06\n",
      "167 3.6313726923253853e-06\n",
      "168 3.431872301007388e-06\n",
      "169 3.262786549385055e-06\n",
      "170 3.1593294806953054e-06\n",
      "171 3.0061132747505326e-06\n",
      "172 2.8984832169953734e-06\n",
      "173 2.751830379565945e-06\n",
      "174 2.6615373371896567e-06\n",
      "175 2.5838264718913706e-06\n",
      "176 2.5365293367940467e-06\n",
      "177 2.426613718853332e-06\n",
      "178 2.374359382883995e-06\n",
      "179 2.2823830931884004e-06\n",
      "180 2.250496436317917e-06\n",
      "181 2.157486733267433e-06\n",
      "182 2.0571746972564142e-06\n",
      "183 1.996648961721803e-06\n",
      "184 1.964229568329756e-06\n",
      "185 1.9043710608457332e-06\n",
      "186 1.8653038296179147e-06\n",
      "187 1.8020361949311337e-06\n",
      "188 1.7909510461322498e-06\n",
      "189 1.739870185701875e-06\n",
      "190 1.6852293356350856e-06\n",
      "191 1.6714423054509098e-06\n",
      "192 1.627990059205331e-06\n",
      "193 1.5737616649857955e-06\n",
      "194 1.5611215076205553e-06\n",
      "195 1.5180376067291945e-06\n",
      "196 1.4967686183808837e-06\n",
      "197 1.437422952221823e-06\n",
      "198 1.3946587387181353e-06\n",
      "199 1.388870032315026e-06\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(200):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5374  1.1459  0.5080  0.2277\n",
      " 1.0326 -1.5230 -1.2499 -0.7892\n",
      "-0.4449 -2.1612  1.0032  1.3698\n",
      " 0.2738 -0.0921 -0.7533 -0.8807\n",
      " 0.0855  0.1675  0.2585 -0.6481\n",
      " 0.4286  0.0183  1.0202 -1.8127\n",
      "-0.7092  1.6531 -2.0067 -1.0229\n",
      "-0.1900  0.4437 -2.5814 -0.3400\n",
      "-1.3135  0.8108  1.4490 -1.3611\n",
      "-0.0259  0.8712  0.7864  0.1123\n",
      "-0.2024 -0.6729 -1.9330 -0.4568\n",
      " 0.3092  0.2651  0.1062  1.2880\n",
      " 0.7365  2.1592 -0.3872  0.5021\n",
      "-0.2911  0.5525 -1.0399 -1.0132\n",
      "-1.4623  0.8358  0.7077  1.6616\n",
      " 0.0307  0.7101 -0.6586 -1.9216\n",
      " 3.5645 -0.6676 -0.0563 -1.7623\n",
      " 1.9317 -0.5456  0.1401 -0.8864\n",
      "-0.4066 -1.0957  2.1662  0.0614\n",
      " 0.9462  0.2138  0.4221  0.8907\n",
      "-0.9819 -1.8990 -0.6634 -0.4846\n",
      "-0.4006 -1.3969 -0.6194 -1.2223\n",
      "-1.0976 -1.9377 -1.3791 -0.3372\n",
      " 1.4102  0.2370 -1.0349  0.2238\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      " Variable containing:\n",
      " 0.5374  1.1461  0.5080  0.2279\n",
      " 1.0326 -1.5230 -1.2499 -0.7892\n",
      "-0.4448 -2.1612  1.0033  1.3698\n",
      " 0.2739 -0.0920 -0.7533 -0.8807\n",
      " 0.0855  0.1674  0.2583 -0.6480\n",
      " 0.4285  0.0186  1.0200 -1.8125\n",
      "-0.7091  1.6531 -2.0067 -1.0228\n",
      "-0.1899  0.4436 -2.5812 -0.3401\n",
      "-1.3135  0.8109  1.4490 -1.3611\n",
      "-0.0259  0.8713  0.7865  0.1122\n",
      "-0.2025 -0.6731 -1.9331 -0.4569\n",
      " 0.3090  0.2652  0.1061  1.2881\n",
      " 0.7366  2.1591 -0.3872  0.5020\n",
      "-0.2911  0.5526 -1.0399 -1.0133\n",
      "-1.4621  0.8358  0.7079  1.6615\n",
      " 0.0307  0.7103 -0.6586 -1.9214\n",
      " 3.5645 -0.6677 -0.0561 -1.7623\n",
      " 1.9318 -0.5458  0.1403 -0.8863\n",
      "-0.4067 -1.0958  2.1662  0.0613\n",
      " 0.9459  0.2138  0.4218  0.8906\n",
      "-0.9818 -1.8994 -0.6633 -0.4847\n",
      "-0.4006 -1.3967 -0.6196 -1.2222\n",
      "-1.0977 -1.9378 -1.3792 -0.3373\n",
      " 1.4102  0.2369 -1.0350  0.2239\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-06 *\n",
       "  1.3889\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn the Autograd module in next section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
