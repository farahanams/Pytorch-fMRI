{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04 PyTorch optim and Intro to Convnets\n",
    "=====================\n",
    "### Date: Jan 12 2018\n",
    "### Author: Farahana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have manipulated **.data** from computed module training to do weights updates.\n",
    "The optim package has simplified this step by providing many types of optimizers such as *Adam*, *AdaGrad* and *RMSPRop*.\n",
    "\n",
    "What we have learnt;\n",
    "\n",
    "1. **Tensor** : multi-dimensional array, just like numpy that utilized GPU \n",
    "2. **autograd.Variable**  : basically to initialize the input and outputs and automate the gradient (dy/dt) computations\n",
    "3. **nn**        : full of neural networks type of layers and loss functions\n",
    "    \n",
    "Now, the next step is to utilize the **optim** package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the previous example and re-define it using **optim**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the example\n",
    "N, D_in, H, D_out = 24, 1000, 100, 4\n",
    "learning_rate = 1e-3\n",
    "dtype = tc.cuda.FloatTensor\n",
    "\n",
    "### autograd.Variable ###\n",
    "x = Variable(tc.randn(N, D_in).type(dtype), requires_grad=False) # input\n",
    "y = Variable(tc.randn(N, D_out).type(dtype), requires_grad=False) # output\n",
    "\n",
    "### nn.Module ###\n",
    "model = nn.Sequential(nn.Linear(D_in,H), nn.ReLU(), nn.Linear(H, D_out)).cuda() \n",
    "loss_fn = nn.MSELoss(size_average=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose and define the optimizer. For now, let us try with Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optim modules\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optmizer module has its own default hyper-parameters that we could change while defining:\n",
    "    \n",
    "    tc.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 102.30847930908203\n",
      "100 0.000534254009835422\n",
      "200 1.150724493470534e-08\n",
      "300 6.961792253790122e-13\n",
      "400 4.184430579812215e-13\n",
      "500 3.8920255907015644e-13\n",
      "600 6.22668583361019e-13\n",
      "700 5.70835045898832e-13\n",
      "800 9.910544607194538e-13\n",
      "900 9.752892937697766e-13\n",
      "1000 1.5639156636382268e-12\n",
      "1100 1.2753686995381486e-12\n",
      "1200 1.573019492440153e-12\n",
      "1300 1.9269724704784608e-12\n",
      "1400 2.28396468404668e-12\n",
      "1500 3.328087805343216e-12\n",
      "1600 8.167536091896466e-12\n",
      "1700 1.2115213394281454e-05\n",
      "1800 7.141022773105021e-10\n",
      "1900 0.00040634864126332104\n"
     ]
    }
   ],
   "source": [
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Print loss every 100 epochs\n",
    "    if (t)%100 == 0:\n",
    "        print(t, loss.data[0])\n",
    "\n",
    "    # zeroing the gradients each time before backward computation using initialized optimizer. \n",
    "    # Replacing the 'model.zero_grad()'\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Rather than using parameters.data, we just call optimizer to update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we will check the expected vs predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6983 -1.4299  1.2260  0.4276\n",
      " 0.0464 -0.3840 -0.1521  1.7557\n",
      " 0.3922 -1.5628  1.5454 -1.3816\n",
      "-0.3886  1.1611  0.6743  1.2763\n",
      " 0.1159  0.6454 -0.1520 -0.5812\n",
      " 1.5397  1.2766 -0.7831  1.1301\n",
      " 0.9382 -1.3022  0.0480  0.1792\n",
      "-0.8961  0.9348 -1.1036  1.2779\n",
      "-0.2001  0.6951  0.9331  1.8725\n",
      " 0.6215  1.4529 -0.2044 -0.5818\n",
      "-0.7041 -1.5773 -0.6432  0.6037\n",
      " 0.3944  1.0887 -0.7971 -2.1805\n",
      "-0.7417  0.2959 -0.2065 -0.1824\n",
      "-2.9820  1.5765  1.1189  0.0981\n",
      " 0.3418  0.1637  0.7485  0.6669\n",
      "-0.0893 -1.0414  0.2990 -0.4111\n",
      "-0.5233  0.0322 -0.6136 -0.2325\n",
      "-2.3458  0.6883 -0.3638 -0.8164\n",
      "-0.3856  0.4345  1.3909 -1.3805\n",
      " 1.2436 -2.2302  1.1033  1.4108\n",
      " 0.1344 -0.7789 -0.1577 -0.6173\n",
      "-1.0368  0.0850 -0.9124 -0.7727\n",
      "-0.1032  0.8354  0.4000  0.0246\n",
      " 0.7474  0.1244 -1.2696 -0.7908\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      " Variable containing:\n",
      " 0.6983 -1.4299  1.2260  0.4276\n",
      " 0.0464 -0.3840 -0.1521  1.7557\n",
      " 0.3922 -1.5628  1.5454 -1.3816\n",
      "-0.3886  1.1611  0.6743  1.2763\n",
      " 0.1158  0.6457 -0.1518 -0.5812\n",
      " 1.5397  1.2766 -0.7831  1.1301\n",
      " 0.9382 -1.3022  0.0480  0.1792\n",
      "-0.8961  0.9348 -1.1036  1.2779\n",
      "-0.2001  0.6951  0.9331  1.8725\n",
      " 0.6215  1.4529 -0.2044 -0.5818\n",
      "-0.7041 -1.5773 -0.6431  0.6037\n",
      " 0.3943  1.0889 -0.7969 -2.1805\n",
      "-0.7418  0.2959 -0.2064 -0.1824\n",
      "-2.9820  1.5765  1.1189  0.0981\n",
      " 0.3418  0.1637  0.7485  0.6669\n",
      "-0.0894 -1.0413  0.2991 -0.4111\n",
      "-0.5233  0.0322 -0.6136 -0.2325\n",
      "-2.3458  0.6883 -0.3637 -0.8164\n",
      "-0.3857  0.4345  1.3909 -1.3805\n",
      " 1.2436 -2.2302  1.1033  1.4108\n",
      " 0.1342 -0.7787 -0.1574 -0.6172\n",
      "-1.0368  0.0850 -0.9124 -0.7727\n",
      "-0.1033  0.8354  0.4000  0.0246\n",
      " 0.7474  0.1244 -1.2697 -0.7908\n",
      "[torch.cuda.FloatTensor of size 24x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have optimized the example in the most simplest implementation.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network inside a class  using MNIST dataset\n",
    "We will use **nn.Functional** where the modules have no trainable or configurable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us define another complex neural network model convolution layers or also known as CNN. \n",
    "* The *initialization* or *constructor* is used to initialize the convolutional and fully connected layers\n",
    "* And the **forward** object is another way to define nn similar to **nn.Sequential** definition.\n",
    "\n",
    "The convolution layers for 2D has a default parameters too.\n",
    "\n",
    "`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    " dilation=1, groups=1, bias=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d (16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=1568, out_features=10)\n",
      "  (fc2): Linear(in_features=10, out_features=84)\n",
      "  (fc3): Linear(in_features=84, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*32, 10)\n",
    "        self.fc2 = nn.Linear(10, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input is the conv2d with 2by2 kernels, and output is max_pool\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2)) \n",
    "        # input is the max_pool with scalar number, the conv2d will be on scalar\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features    \n",
    "            \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d (1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d (16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=1568, out_features=10)\n",
       "  (fc2): Linear(in_features=10, out_features=84)\n",
       "  (fc3): Linear(in_features=84, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learnt that backpropagation is done autonomously by Autograd. \n",
    "Thus, no backward definition needed in the **Net()** module.\n",
    "\n",
    "What is still missing;\n",
    "* Input and expected output\n",
    "* Loss and optimizer definition\n",
    "* loss.backward()\n",
    "* Optimization step\n",
    "* Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data loading\n",
    "We will define two types of datasets; train and test. In both types, there will be images as input and labels as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when train=True, it is training.pt\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download=True)\n",
    "\n",
    "# when train=False, it is test.pt\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **tc.util** for data loading in PyTorch. However, we will learn in next part extensively.\n",
    "\n",
    "`tc.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=<function default_collate at 0x7fa6d122d9d8>, pin_memory=False, drop_last=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train set\n",
    "train_set = tc.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# For test set\n",
    "test_set = tc.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and optimizer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training step\n",
    "Where loss.backward(), optimization and traininig involve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Iter [100/600] Loss: 0.4800\n",
      "Epoch [1/10], Iter [200/600] Loss: 0.2308\n",
      "Epoch [1/10], Iter [300/600] Loss: 0.3248\n",
      "Epoch [1/10], Iter [400/600] Loss: 0.2311\n",
      "Epoch [1/10], Iter [500/600] Loss: 0.0498\n",
      "Epoch [1/10], Iter [600/600] Loss: 0.0796\n",
      "Epoch [2/10], Iter [100/600] Loss: 0.0613\n",
      "Epoch [2/10], Iter [200/600] Loss: 0.1252\n",
      "Epoch [2/10], Iter [300/600] Loss: 0.0481\n",
      "Epoch [2/10], Iter [400/600] Loss: 0.0391\n",
      "Epoch [2/10], Iter [500/600] Loss: 0.0684\n",
      "Epoch [2/10], Iter [600/600] Loss: 0.0306\n",
      "Epoch [3/10], Iter [100/600] Loss: 0.0101\n",
      "Epoch [3/10], Iter [200/600] Loss: 0.0537\n",
      "Epoch [3/10], Iter [300/600] Loss: 0.1787\n",
      "Epoch [3/10], Iter [400/600] Loss: 0.1271\n",
      "Epoch [3/10], Iter [500/600] Loss: 0.0460\n",
      "Epoch [3/10], Iter [600/600] Loss: 0.0911\n",
      "Epoch [4/10], Iter [100/600] Loss: 0.0358\n",
      "Epoch [4/10], Iter [200/600] Loss: 0.0802\n",
      "Epoch [4/10], Iter [300/600] Loss: 0.0788\n",
      "Epoch [4/10], Iter [400/600] Loss: 0.0429\n",
      "Epoch [4/10], Iter [500/600] Loss: 0.0248\n",
      "Epoch [4/10], Iter [600/600] Loss: 0.0278\n",
      "Epoch [5/10], Iter [100/600] Loss: 0.0151\n",
      "Epoch [5/10], Iter [200/600] Loss: 0.0584\n",
      "Epoch [5/10], Iter [300/600] Loss: 0.0475\n",
      "Epoch [5/10], Iter [400/600] Loss: 0.0159\n",
      "Epoch [5/10], Iter [500/600] Loss: 0.0588\n",
      "Epoch [5/10], Iter [600/600] Loss: 0.0256\n",
      "Epoch [6/10], Iter [100/600] Loss: 0.0689\n",
      "Epoch [6/10], Iter [200/600] Loss: 0.1167\n",
      "Epoch [6/10], Iter [300/600] Loss: 0.0308\n",
      "Epoch [6/10], Iter [400/600] Loss: 0.0068\n",
      "Epoch [6/10], Iter [500/600] Loss: 0.0104\n",
      "Epoch [6/10], Iter [600/600] Loss: 0.0362\n",
      "Epoch [7/10], Iter [100/600] Loss: 0.0484\n",
      "Epoch [7/10], Iter [200/600] Loss: 0.0310\n",
      "Epoch [7/10], Iter [300/600] Loss: 0.0066\n",
      "Epoch [7/10], Iter [400/600] Loss: 0.0050\n",
      "Epoch [7/10], Iter [500/600] Loss: 0.0457\n",
      "Epoch [7/10], Iter [600/600] Loss: 0.0652\n",
      "Epoch [8/10], Iter [100/600] Loss: 0.0077\n",
      "Epoch [8/10], Iter [200/600] Loss: 0.0092\n",
      "Epoch [8/10], Iter [300/600] Loss: 0.0472\n",
      "Epoch [8/10], Iter [400/600] Loss: 0.0343\n",
      "Epoch [8/10], Iter [500/600] Loss: 0.0179\n",
      "Epoch [8/10], Iter [600/600] Loss: 0.0189\n",
      "Epoch [9/10], Iter [100/600] Loss: 0.0038\n",
      "Epoch [9/10], Iter [200/600] Loss: 0.0062\n",
      "Epoch [9/10], Iter [300/600] Loss: 0.0018\n",
      "Epoch [9/10], Iter [400/600] Loss: 0.0312\n",
      "Epoch [9/10], Iter [500/600] Loss: 0.0119\n",
      "Epoch [9/10], Iter [600/600] Loss: 0.0445\n",
      "Epoch [10/10], Iter [100/600] Loss: 0.0402\n",
      "Epoch [10/10], Iter [200/600] Loss: 0.0229\n",
      "Epoch [10/10], Iter [300/600] Loss: 0.0064\n",
      "Epoch [10/10], Iter [400/600] Loss: 0.0539\n",
      "Epoch [10/10], Iter [500/600] Loss: 0.1003\n",
      "Epoch [10/10], Iter [600/600] Loss: 0.0497\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # define input as images, and output as labels into Variable to get autograd automatically\n",
    "    for i, (images, labels) in enumerate(train_set):\n",
    "        x = Variable(images).cuda()\n",
    "        y = Variable(labels).cuda()\n",
    "        \n",
    "        y_pred = net(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Print loss every 100 epochs\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d (1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d (16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=1568, out_features=10)\n",
       "  (fc2): Linear(in_features=10, out_features=84)\n",
       "  (fc3): Linear(in_features=84, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_set:\n",
    "    images = Variable(images).cuda()\n",
    "    outputs = net(images)\n",
    "    _, predicted = tc.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
